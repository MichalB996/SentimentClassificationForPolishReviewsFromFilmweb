{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a memory-friendly iterator\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                print(os.path.join(self.dirname, fname))\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "root = '' # root of your data \n",
    "polishReviews = open(root, 'r',errors='ignore',encoding=\"utf8\")\n",
    "lines = polishReviews.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning and preprocessing. Some variants: \n",
    "# lower\n",
    "# lower + punctuation cleaning\n",
    "# changing near words \n",
    "#----------------------------\n",
    "posReviews = ''\n",
    "negReviews =''\n",
    "mostCommonWords = ['i','w','z', 'na', 'je', 'że', 'do', 'ale', 'mnie','ten', 'tego', 'tak', 'kto','wiem','ja', 'od','fil', 'któr','który']\n",
    "for line in lines:\n",
    "    line = line.lower()\n",
    "    newLine = ' '.join(line.split())\n",
    "    newLine = newLine.replace('\"','')\n",
    "    review = newLine[:(len(newLine) - 2)]\n",
    "    gradeLine = newLine[(len(newLine) - 2):]\n",
    "    if ',' in gradeLine:\n",
    "        gradeLine = gradeLine.replace(',','')\n",
    "    if int(gradeLine) > 5:\n",
    "        review = review.translate(str.maketrans('','',string.punctuation)).split()        \n",
    "        review = ' '.join(review)\n",
    "        posReviews += review\n",
    "        posReviews += '\\n'\n",
    "\n",
    "    else:\n",
    "        review = review.translate(str.maketrans('','',string.punctuation)).split()\n",
    "        review = ' '.join(review)\n",
    "        negReviews += review\n",
    "        negReviews += '\\n'\n",
    "#posReviewsSplitted = posReviews.split()\n",
    "#negReviewsSplitted = negReviews.split()\n",
    "        \n",
    "for i in mostCommonWords:\n",
    "    if str(i) in posReviewsSplitted:\n",
    "        posReviewsSplitted.remove(str(i))\n",
    "    if str(i) in negReviewsSplitted:\n",
    "        negReviewsSplitted.remove(str(i))\n",
    "    \n",
    "#cleanedPos = ' '.join(posReviewsSplitted)\n",
    "#cleanedNeg = ' '.join(negReviewsSplitted)\n",
    "#print(cleanedPos)\n",
    "for i in hapaxLegomena:\n",
    "    if str(i) in posReviewsSplitted:\n",
    "        posReviewsSplitted.remove(str(i))\n",
    "    if str(i) in negReviewsSplitted:\n",
    "        negReviewsSplitted.remove(str(i))\n",
    "posResult = open(r'C:\\Users\\Michał\\Desktop\\iet\\2_semestr\\PJN\\2\\allPosReviewsLowerWithoutPunctuationWithoutHapaxWithoutCommon.txt', 'w',errors='ignore',encoding=\"utf8\")\n",
    "posResult.write(posReviews)\n",
    "posResult.close()\n",
    "negResult = open(r'C:\\Users\\Michał\\Desktop\\iet\\2_semestr\\PJN\\2\\allNegReviewsLowerWithoutPunctuationWithoutHapaxWithoutCommon.txt', 'w',errors='ignore',encoding=\"utf8\")\n",
    "negResult.write(negReviews)\n",
    "negResult.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make set of mostCommonWords and HapaXLegomena\n",
    "from collections import Counter\n",
    "wholeData = posReviews+'\\n'+negReviews\n",
    "countWords = Counter(wholeData.split())\n",
    "hapaxLegomena = []\n",
    "totalWords = len(countWords)\n",
    "sortedWords = countWords.most_common(totalWords)\n",
    "for i in sortedWords:\n",
    "    if i[1] == 1:\n",
    "        hapaxLegomena.append(i[0])\n",
    "    \n",
    "words = ''\n",
    "counter = 1\n",
    "for word in sortedWords:\n",
    "    counter += 1\n",
    "    if counter == 200:\n",
    "        break\n",
    "    words += word[0] +'\\n'\n",
    "#mostCommonWords.write(words)\n",
    "#mostCommonWords.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '' #root to data to process\n",
    "file = open(root,mode='r', errors='ignore',encoding=\"utf8\")\n",
    "lines = file.readlines()\n",
    "whole = ''\n",
    "for line in lines:\n",
    "    whole += line.replace('ą','a').replace('ć','c').replace('ę','e').replace('ń','n').replace('ł','l').replace('ó','o').replace('ś','s').replace('ź','z').replace('ż','z').replace('\\n','')+'\\n'    \n",
    "file = open(root,mode='w', errors='ignore',encoding=\"utf8\")   \n",
    "file.write(whole)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json from each line (each line is a review)\n",
    "root =''\n",
    "file = open(root ,errors='ignore',encoding=\"utf8\")\n",
    "lines = file.readlines()\n",
    "wholeJson =''\n",
    "\n",
    "for line in lines:    \n",
    "    json ='{\"text\":'\n",
    "    body = '['\n",
    "    for word in line.split():\n",
    "        body += '\"' + word +'\",'\n",
    "    body = body[0:-1]\n",
    "    body += ']'\n",
    "    json += body\n",
    "    json += ', \"label\": \"neg\"}\\n'\n",
    "    wholeJson += json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open(root, 'w',errors='ignore',encoding=\"utf8\")\n",
    "result.write(wholeJson)\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of word2vect model\n",
    "\n",
    "model = gensim.models.Word2Vec(size = 100, window = 5)\n",
    "model.build_vocab(reviews)\n",
    "model.save(\"polishWord2VectModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepraing vectors to torxtext format\n",
    "\n",
    "word2vect = ''\n",
    "\n",
    "for item in model.wv.vocab:\n",
    "    stri = str(item)+str(' ')+str(model.wv[item]).replace('[','').replace(']','').replace('\\n','')\n",
    "    stri = re.sub(' +', ' ', stri)\n",
    "    if len(stri.split()) > 101:\n",
    "        print(str(item))\n",
    "    word2vect += stri+'\\n'\n",
    "resFile = open(r'./polishVectors.txt', 'w',errors='ignore',encoding=\"utf8\")    \n",
    "resFile.write(word2vect)\n",
    "resFile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r'./PolishWithoutPunctuationWithoutHapaxWihoutPolish\\\\PosReviewsLowerWithoutPunctuationWithoutHapaxLegomenaPreparedWithoutPolishCharactersPrepared.json'\n",
    "file = open(root, 'r',errors='ignore',encoding=\"utf8\") \n",
    "lines = file.readlines()\n",
    "whole = ''\n",
    "for line in lines:\n",
    "    line = line.replace('}','}\\n')\n",
    "    whole += line\n",
    "print(whole)\n",
    "file = open(root, 'w',errors='ignore',encoding=\"utf8\") \n",
    "file.write(whole)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json checker - check if the json is in proper format\n",
    "import json \n",
    "polishReviews = open(root, 'r', errors='ignore', encoding=\"utf8\")\n",
    "lines = polishReviews.readlines()\n",
    "counter = 0\n",
    "for line in lines:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    json.loads(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
